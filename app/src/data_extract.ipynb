{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PyPDF2 import PdfFileReader\n",
    "import os\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract information from pdf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_pdf(path):\n",
    "    pdf=PdfFileReader(path, strict=False)\n",
    "    number_pages=pdf.getNumPages()\n",
    "    content=''\n",
    "    for i in range(number_pages):\n",
    "        content+=pdf.getPage(i).extractText()\n",
    "    \n",
    "    content=content.replace('\\n','. ').replace('\\n\\n','. ')\n",
    "    content=content.replace('\\t',' ')\n",
    "    \n",
    "    return content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get path to each pdf file stored in folder  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_path_pdf(path_folder):\n",
    "    path_pdf=[]\n",
    "    for dirname,_,filenames in os.walk(path_folder):\n",
    "        for filename in filenames:\n",
    "            path_pdf.append(os.path.join(dirname,filename))\n",
    "    return path_pdf\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_JD=\"./static/files_JD/\"\n",
    "path_2_pdfJD=get_path_pdf(path_JD)\n",
    "content_JD=extract_pdf(path_2_pdfJD[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_CV=\"./static/files_CV/\"\n",
    "path_2_pdfCV=get_path_pdf(path_CV)\n",
    "content_CV=[]\n",
    "for p in path_2_pdfCV:\n",
    "    content_CV.append(extract_pdf(p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Spacy to extract skill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.pipeline import EntityRuler\n",
    "from spacy import displacy\n",
    "import jsonlines\n",
    "import spacy\n",
    "import re \n",
    "import pyperclip\n",
    "from sympy import true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_path=\"skill_paterns.jsonl\"\n",
    "\n",
    "with jsonlines.open(json_path) as f:\n",
    "    entities=[line[\"label\"].upper() for line in f.iter()]\n",
    "\n",
    "# spacy model\n",
    "nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<spacy.pipeline.entityruler.EntityRuler at 0x1a1f93ec4c0>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\")\n",
    "ruler.from_disk(json_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=nlp(content_CV[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sent in (list(doc.sents)):\n",
    "    tmp=sent.text.split(\" \")\n",
    "    for wolrd in tmp:\n",
    "        wolrd=wolrd.lower()\n",
    "        if(wolrd=='experience'):\n",
    "            print(sent)\n",
    "            print('===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PERSONAL INFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_skill(content_CV):\n",
    "    doc=nlp(content_CV)\n",
    "    skill_list=[]\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='SKILL':\n",
    "            skill_list.append(ent.text)\n",
    "    skill_list=list(set(skill_list))\n",
    "    return skill_list\n",
    "\n",
    "def get_GPA(content_CV):\n",
    "    doc=nlp(content_CV)\n",
    "    sent=list(doc.sents)\n",
    "    gpa=''\n",
    "    for s in sent:\n",
    "        s=s.text\n",
    "        if s.find('GPA') !=-1 :\n",
    "            gba_pos= s.find('GPA') \n",
    "            for i in range (gba_pos+3,gba_pos+8):\n",
    "                if (s[i] >'0' and s[i]<'9' or s[i]=='.'):\n",
    "                    gpa+=s[i]\n",
    "    if gpa=='':\n",
    "        return None\n",
    "    else:\n",
    "        return gpa\n",
    "\n",
    "def get_experience(content_CV):\n",
    "    '''\n",
    "    Return\\n\n",
    "        None if no experience\\n\n",
    "        List of experience if exist\n",
    "    '''\n",
    "    doc=nlp(content_CV)\n",
    "    sent=list(doc.sents)\n",
    "    final_res=''\n",
    "    ex=[]\n",
    "    for s in sent:\n",
    "        s_text=s.text.split(' ')\n",
    "        flag=False\n",
    "        for i in s_text:\n",
    "            i=i.lower()\n",
    "            if i=='experience':\n",
    "                flag=True\n",
    "        if flag==True:\n",
    "            ex.append([s.text])\n",
    "    if (len(ex)==0):\n",
    "        final_res=None\n",
    "    else:\n",
    "        final_res=ex\n",
    "    return final_res\n",
    "\n",
    "def get_education(content_CV):\n",
    "    doc=nlp(content_CV)\n",
    "    sent=list(doc.sents)\n",
    "    edu=''\n",
    "    pos_edu=-1\n",
    "    pos_edu_end=-1\n",
    "    for s in sent:\n",
    "        s_text=s.text.lower()\n",
    "        flag=False\n",
    "        if s_text.find('education')!=-1 or s_text.find('university')!=-1 or s_text.find('academic')!=-1 or s_text.find('bachelor ')!=-1:\n",
    "            edu =s.text\n",
    "            if s_text.find('education')!=-1 :\n",
    "                pos_edu=s_text.find('education')+9\n",
    "            if s_text.find('academic') !=-1 :\n",
    "                pos_edu=s_text.find('academic')\n",
    "            if s_text.find('university') !=-1 :\n",
    "                pos_edu=s_text.find('university')\n",
    "            else:\n",
    "                pos_edu=s_text.find('bachelor ')\n",
    "            \n",
    "            pos_edu_end=s_text.find('. ',pos_edu)\n",
    "            edu=edu[pos_edu:pos_edu_end]\n",
    "    if edu=='':\n",
    "        return None\n",
    "    else:\n",
    "        return edu\n",
    "\n",
    "def get_phone(content_CV):\n",
    "    '''\n",
    "    Format for Phone-Numbers:\n",
    "    134567890\n",
    "    123-456-7890\n",
    "    (123)(456)(7890)\n",
    "    123.456.7890\n",
    "    (123)(456-7890)\n",
    "    '''\n",
    "    phone= re.compile(r'''\n",
    "        (\\d{3}|\\(\\d{3}\\))    \n",
    "        (\\s|-|\\.)?           \n",
    "        (\\d{3}|\\(\\d{3}\\))    \n",
    "        (\\s|-|\\.)?           \n",
    "        (\\d{4}|\\(\\d{4}\\))    \n",
    "    ''', re.VERBOSE) \n",
    "\n",
    "    matchs=phone.finditer(content_CV)\n",
    "    res = ''\n",
    "    for match in matchs:\n",
    "        res+=match.group(0)\n",
    "    if res=='':\n",
    "        return None\n",
    "    else:\n",
    "        return res\n",
    "\n",
    "def get_link(link,content_CV):\n",
    "    matchs=link.finditer(content_CV)\n",
    "    res=''\n",
    "    for match in matchs:\n",
    "        res+=match.group(0)\n",
    "    if res=='':\n",
    "        return None\n",
    "    else:\n",
    "        return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.tokens import Span\n",
    "import dateparser\n",
    "def expand_person_entities(doc):\n",
    "    new_ents = []\n",
    "    for ent in doc.ents:\n",
    "        # Only check for title if it's a person and not the first token\n",
    "        if ent.label_ == \"PERSON\":\n",
    "            if ent.start != 0:\n",
    "                # if person preceded by title, include title in entity\n",
    "                prev_token = doc[ent.start - 1]\n",
    "                if prev_token.text in (\"Dr\", \"Dr.\", \"Mr\", \"Mr.\", \"Ms\", \"Ms.\"):\n",
    "                    new_ent = Span(doc, ent.start - 1, ent.end, label=ent.label)\n",
    "                    new_ents.append(new_ent)\n",
    "                else:\n",
    "                    # if entity can be parsed as a date, it's not a person\n",
    "                    if dateparser.parse(ent.text) is None:\n",
    "                        new_ents.append(ent) \n",
    "        else:\n",
    "            new_ents.append(ent)\n",
    "    doc.ents = new_ents\n",
    "    return doc\n",
    "\n",
    "# Add the component after the named entity recognizer\n",
    "# # nlp.remove_pipe('expand_person_entities')\n",
    "# @Language.component(\"expand_person_entities\")\n",
    "# nlp.add_pipe(\"expand_person_entities\", after='ner')\n",
    "\n",
    "# doc = nlp(content_CV[0])\n",
    "# [(ent.text, ent.label_) for ent in doc.ents if ent.label_=='PERSON']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'lÃ  sinh'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from spacy.matcher import Matcher\n",
    "# initialize matcher with a vocab\n",
    "\n",
    "def extract_name(content_CV):\n",
    "    matcher = Matcher(nlp.vocab)\n",
    "    doc=nlp(content_CV)\n",
    "    error_key=['city','school','university','freelancer','building']\n",
    "    warning_key=['curriculum vita']\n",
    "    cut_pos=-1\n",
    "    # Fisrt way: using ner in spacy\n",
    "    for ent in doc.ents:\n",
    "        if ent.label_=='PERSON':\n",
    "            txt=ent.text\n",
    "            txt_lower=txt.lower()\n",
    "            flag=True\n",
    "            for i in error_key:\n",
    "                if i in txt_lower:\n",
    "                    flag=False\n",
    "            for j in warning_key:\n",
    "                if j in txt_lower:\n",
    "                    cut_pos=txt_lower.find(j)\n",
    "                    txt=txt[:cut_pos]\n",
    "            if flag==True:\n",
    "                return txt\n",
    "            else:\n",
    "                break\n",
    "    # Second way: First name and Last name are always Proper Nouns\n",
    "    pattern = [\n",
    "        {'POS': 'PROPN'},{'POS': 'PROPN'},\n",
    "        ]\n",
    "\n",
    "    matcher.add('NAME',[pattern])\n",
    "    matches = matcher(doc)\n",
    "    for match_id, start, end in matches:\n",
    "        span = doc[start:end]\n",
    "        return span.text\n",
    "    return None\n",
    "\n",
    "extract_name(content_CV[5])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check Accuracy of name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_data = json.load(open('./data/training/train_data.json','rb')) \n",
    "\n",
    "def name_acc(train_data):\n",
    "    list_an=[]\n",
    "    list_txt=[]\n",
    "    name=[]\n",
    "\n",
    "    for text,annotation in train_data:\n",
    "        list_an.append(list(annotation.values()))\n",
    "        list_txt.append(text)\n",
    "\n",
    "    target_name=[]\n",
    "    predic_name=[]\n",
    "\n",
    "    similar=0\n",
    "    for i in range(len(list_an)):\n",
    "        t=''\n",
    "        p=''\n",
    "        p=extract_name(list_txt[i])\n",
    "        predic_name.append(p)\n",
    "        for j in range(len(list_an[i][0])):\n",
    "            if list_an[i][0][j][2]=='Name':\n",
    "                t=list_txt[i][list_an[i][0][j][0]:list_an[i][0][j][1]]\n",
    "                target_name.append(t)\n",
    "        if t==p:\n",
    "            similar+=1\n",
    "    return (similar/len(list_an)*100)    \n",
    "# name_acc(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load accuracy name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model=spacy.load('nlp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using sklearn to calculate the cosine similarity score of CV and JD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similar(list_CV,JD):\n",
    "    df_rank_match=pd.DataFrame(columns=['ID','Percent_Match_JD'])\n",
    "    for i in range(len(list_CV)):\n",
    "        test=[list_CV[i],JD]\n",
    "        cv = CountVectorizer()\n",
    "        count_matrix= cv.fit_transform(test)\n",
    "        match=cosine_similarity(count_matrix)        \n",
    "        row = {\n",
    "            'ID':i,\n",
    "            'Percent_Match_JD': round(match[0][1]*100,2)\n",
    "        }\n",
    "        new_df=pd.DataFrame([row])\n",
    "        df_rank_match=pd.concat([df_rank_match,new_df],axis=0,ignore_index=True)\n",
    "    return df_rank_match \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final extract and save to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [57], line 50\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[39mreturn\u001b[39;00m df_information\n\u001b[0;32m     40\u001b[0m \u001b[39m# list_CV=[]\u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[39m# count=0\u001b[39;00m\n\u001b[0;32m     42\u001b[0m \u001b[39m# train_data = json.load(open('./data/training/train_data.json','rb')) \u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[39m#     if count==50:\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[39m#         break\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m df_detail\u001b[39m=\u001b[39mextract_all_data(content_CV,content_JD)\n\u001b[0;32m     51\u001b[0m df_detail\n",
      "Cell \u001b[1;32mIn [57], line 31\u001b[0m, in \u001b[0;36mextract_all_data\u001b[1;34m(list_content_CV, content_JD)\u001b[0m\n\u001b[0;32m     19\u001b[0m link\u001b[39m.\u001b[39mappend(get_link(web,content))\n\u001b[0;32m     20\u001b[0m link\u001b[39m.\u001b[39mappend(get_link(linkedin,content))\n\u001b[0;32m     22\u001b[0m row\u001b[39m=\u001b[39m{\n\u001b[0;32m     23\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mID\u001b[39m\u001b[39m'\u001b[39m:i,\n\u001b[0;32m     24\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mName\u001b[39m\u001b[39m'\u001b[39m:extract_name(content),\n\u001b[0;32m     25\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEmail\u001b[39m\u001b[39m'\u001b[39m:get_link(mail,content),\n\u001b[0;32m     26\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mPhone_Number\u001b[39m\u001b[39m'\u001b[39m:get_phone(content),\n\u001b[0;32m     27\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mRelated_Link\u001b[39m\u001b[39m'\u001b[39m:link,\n\u001b[0;32m     28\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mSkill\u001b[39m\u001b[39m'\u001b[39m:get_skill(content),\n\u001b[0;32m     29\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mExperience\u001b[39m\u001b[39m'\u001b[39m:get_experience(content),\n\u001b[0;32m     30\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mEducation\u001b[39m\u001b[39m'\u001b[39m:get_education(content),\n\u001b[1;32m---> 31\u001b[0m         \u001b[39m'\u001b[39m\u001b[39mGPA\u001b[39m\u001b[39m'\u001b[39m:get_GPA(content)\n\u001b[0;32m     32\u001b[0m     }\n\u001b[0;32m     33\u001b[0m new_df\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mDataFrame([row])\n\u001b[0;32m     34\u001b[0m df_information\u001b[39m=\u001b[39mpd\u001b[39m.\u001b[39mconcat([df_information,new_df],axis\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m,ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "Cell \u001b[1;32mIn [50], line 19\u001b[0m, in \u001b[0;36mget_GPA\u001b[1;34m(content_CV)\u001b[0m\n\u001b[0;32m     17\u001b[0m         gba_pos\u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mfind(\u001b[39m'\u001b[39m\u001b[39mGPA\u001b[39m\u001b[39m'\u001b[39m) \n\u001b[0;32m     18\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m (gba_pos\u001b[39m+\u001b[39m\u001b[39m3\u001b[39m,gba_pos\u001b[39m+\u001b[39m\u001b[39m8\u001b[39m):\n\u001b[1;32m---> 19\u001b[0m             \u001b[39mif\u001b[39;00m (s[i] \u001b[39m>\u001b[39m\u001b[39m'\u001b[39m\u001b[39m0\u001b[39m\u001b[39m'\u001b[39m \u001b[39mand\u001b[39;00m s[i]\u001b[39m<\u001b[39m\u001b[39m'\u001b[39m\u001b[39m9\u001b[39m\u001b[39m'\u001b[39m \u001b[39mor\u001b[39;00m s[i]\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m):\n\u001b[0;32m     20\u001b[0m                 gpa\u001b[39m+\u001b[39m\u001b[39m=\u001b[39ms[i]\n\u001b[0;32m     21\u001b[0m \u001b[39mif\u001b[39;00m gpa\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39m'\u001b[39m:\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "def extract_all_data(list_content_CV,content_JD):\n",
    "    df_match=cosine_similar(list_content_CV,content_JD)\n",
    "    extract_df_match=df_match[\"Percent_Match_JD\"]\n",
    "\n",
    "    label=['ID','Name','Email','Phone_Number','Related_Link','Skill','Experience','Education','GPA']\n",
    "\n",
    "    df_information = pd.DataFrame(columns=label) \n",
    "    # bitbucket\n",
    "    mail = re.compile(r'[a-zA-Z0-9-\\.]+@[a-zA-Z-\\.]*\\.(com|edu|net|vn)')\n",
    "    \n",
    "    git = re.compile(r'(gitlab.com/|github.com/)+[a-zA-Z0-9-\\.]*')\n",
    "    web = re.compile(r'(http://|https:// )+[a-zA-Z0-9-\"/\"-\\.]*')\n",
    "    linkedin = re.compile(r'linkedin.com/+[a-zA-Z0-9-\"/\"-\\.]*')\n",
    "\n",
    "    for i in range(len(list_content_CV)):\n",
    "        content=list_content_CV[i]\n",
    "        link=[]\n",
    "        link.append(get_link(git,content))\n",
    "        link.append(get_link(web,content))\n",
    "        link.append(get_link(linkedin,content))\n",
    "        \n",
    "        row={\n",
    "                'ID':i,\n",
    "                'Name':extract_name(content),\n",
    "                'Email':get_link(mail,content),\n",
    "                'Phone_Number':get_phone(content),\n",
    "                'Related_Link':link,\n",
    "                'Skill':get_skill(content),\n",
    "                'Experience':get_experience(content),\n",
    "                'Education':get_education(content),\n",
    "                # 'GPA':get_GPA(content)\n",
    "            }\n",
    "        new_df=pd.DataFrame([row])\n",
    "        df_information=pd.concat([df_information,new_df],axis=0,ignore_index=True)\n",
    "    df_information=df_information.join(extract_df_match)\n",
    "    df_information=df_information.sort_values(by='Percent_Match_JD', ascending=False)\n",
    "    return df_information\n",
    "\n",
    "\n",
    "# list_CV=[]\n",
    "# count=0\n",
    "# train_data = json.load(open('./data/training/train_data.json','rb')) \n",
    "\n",
    "# for text,annotation in train_data:\n",
    "#     count+=1\n",
    "#     list_CV.append(text)\n",
    "#     if count==50:\n",
    "#         break\n",
    "\n",
    "df_detail=extract_all_data(content_CV,content_JD)\n",
    "df_detail"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "231b793e72510f63263cf75731d759da8776d8c74c3ed2dbefa0f0827e58fa6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
