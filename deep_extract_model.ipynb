{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import pickle\n",
    "import random\n",
    "import json\n",
    "from spacy.util import filter_spans\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = json.load(open('./data/training/train_data.json','rb')) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_an=[]\n",
    "\n",
    "for text,annotation in train_data:\n",
    "    list_an.append(list(annotation.values()))\n",
    "\n",
    "# print(len(list_an))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp=spacy.load(\"en_core_web_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.util import filter_spans\n",
    "\n",
    "for txt,annot in train_data:\n",
    "    doc=nlp.make_doc(txt)\n",
    "    # print(list(annot.values()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[587, 669, 'Skills'], [512, 553, 'College Name'], [503, 510, 'Degree'], [177, 193, 'Designation'], [34, 58, 'Companies worked at'], [14, 30, 'Designation'], [0, 13, 'Name']]\n"
     ]
    }
   ],
   "source": [
    "# from spacy.util import filter_spans\n",
    "# from tqdm import tqdm\n",
    "# for sample_training in tqdm(train_data['annotations']):\n",
    "#     txt=sample_training['text']\n",
    "#     labels=sample_training['entities']\n",
    "#     doc=nlp.make_doc(txt)\n",
    "#     for start,end,label in labels:\n",
    "#         print(start,end,label)\n",
    "\n",
    "# doc = nlp(\"This is a sentence.\")\n",
    "# doc2=nlp(\"lalala la ha hu\")\n",
    "# spans = [doc2[0:2],doc2[1:4],doc2[1:3],doc[0:2], doc[0:2], doc[2:4],doc2[0:1]]\n",
    "# filtered = filter_spans(spans)\n",
    "# print(spans)\n",
    "# print(filtered)\n",
    "\n",
    "from spacy.util import filter_spans\n",
    "for sample_training in annotation.values():\n",
    "    print(sample_training)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove overlap entyties\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and (list_an[j][0][i][1]<=list_an[j][0][i+1][1])\n",
    "list_remov=[]\n",
    "for j in range(len(list_an)):\n",
    "    for i in range(len(list_an[j][0])-2):\n",
    "        if((list_an[j][0][i][0] >= list_an[j][0][i+1][0]) ):\n",
    "            print(list_an[j][0][i])\n",
    "            print(list_an[j][0][i+1])\n",
    "            list_remov.append(list_an[j][0][i])       \n",
    "'''(1417, 1423, 'Companies worked at')' and '(1356, 1793, 'Skills')'''\n",
    "len(list_remov)\n",
    "\n",
    "count=0\n",
    "for text, ano in train_data:\n",
    "    #print(ano.values()) \n",
    "    for i in ano.values():\n",
    "        for j in i:\n",
    "            if(j in list_remov):\n",
    "                count+=1\n",
    "                i.remove(j)\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "for text, ano in train_data:\n",
    "    #print(ano.values()) \n",
    "    for i in ano.values():\n",
    "        for j in i:\n",
    "            if(j in list_remov):\n",
    "                print('Ex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('College Name',\n",
       " 'Companies worked at',\n",
       " 'Degree',\n",
       " 'Designation',\n",
       " 'Email Address',\n",
       " 'Location',\n",
       " 'Name',\n",
       " 'Skills',\n",
       " 'Years of Experience')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp=spacy.blank('en')\n",
    "\n",
    "if 'ner' not in nlp.pipe_names:\n",
    "    ner=nlp.create_pipe('ner')\n",
    "    nlp.add_pipe('ner',last=True)\n",
    "\n",
    "for _,annotation in train_data:\n",
    "    for ent in annotation['entities']:\n",
    "        ner.add_label(ent[2])\n",
    "other_pipes=[pipe for pipe in nlp.pipe_names if pipe !='ner']\n",
    "\n",
    "ner.labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOSSES:  {'ner': 136.0509955883026}\n",
      "LOSSES:  {'ner': 2654.4913761019707}\n",
      "LOSSES:  {'ner': 248.00915092229843}\n",
      "LOSSES:  {'ner': 1422.976767539978}\n",
      "LOSSES:  {'ner': 363.04590862989426}\n",
      "LOSSES:  {'ner': 17.793806751898956}\n",
      "LOSSES:  {'ner': 7.735190449739093}\n",
      "LOSSES:  {'ner': 8.0093088534652}\n",
      "LOSSES:  {'ner': 7.9576182922252485}\n",
      "LOSSES:  {'ner': 8.032414857785625}\n",
      "LOSSES:  {'ner': 7.986526742031359}\n",
      "LOSSES:  {'ner': 7.96301423932448}\n",
      "LOSSES:  {'ner': 7.961725410710642}\n",
      "LOSSES:  {'ner': 8.110388010420138}\n",
      "LOSSES:  {'ner': 8.116100307757733}\n",
      "LOSSES:  {'ner': 7.861883743640647}\n",
      "LOSSES:  {'ner': 7.865553314807357}\n",
      "LOSSES:  {'ner': 7.464850562899301}\n",
      "LOSSES:  {'ner': 7.879219527877922}\n",
      "LOSSES:  {'ner': 7.107734176712654}\n",
      "LOSSES:  {'ner': 7.471228843622157}\n",
      "LOSSES:  {'ner': 5.658395021786646}\n",
      "LOSSES:  {'ner': 6.081883213077276}\n",
      "LOSSES:  {'ner': 5.610488586813787}\n",
      "LOSSES:  {'ner': 258.13185499852625}\n",
      "LOSSES:  {'ner': 211.9488467424826}\n",
      "LOSSES:  {'ner': 10.411840020472482}\n",
      "LOSSES:  {'ner': 224.26822998149146}\n",
      "LOSSES:  {'ner': 7.479219615085327}\n",
      "LOSSES:  {'ner': 2.9477392442757324}\n",
      "LOSSES:  {'ner': 6.8712877824945}\n",
      "LOSSES:  {'ner': 6.43535138625972}\n",
      "LOSSES:  {'ner': 5.7272802116515855}\n",
      "LOSSES:  {'ner': 6.30463801994652}\n",
      "LOSSES:  {'ner': 5.637377469038938}\n",
      "LOSSES:  {'ner': 7.921656093585643}\n",
      "LOSSES:  {'ner': 8.315931662713467}\n",
      "LOSSES:  {'ner': 5.99670308253917}\n",
      "LOSSES:  {'ner': 7.420543147955726}\n",
      "LOSSES:  {'ner': 4.534742822293708}\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E103] Trying to set conflicting doc.ents: '(0, 15, 'Name')' and '(0, 4, 'Location')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [53], line 15\u001b[0m\n\u001b[0;32m      9\u001b[0m doc\u001b[39m=\u001b[39mnlp\u001b[39m.\u001b[39mmake_doc(text)\n\u001b[0;32m     11\u001b[0m \u001b[39m# print('BEFORE: ',annotations['entities'])\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[39m# annotations['entities']=filter_spans([annotations['entities']])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[39m# print('AFTER: ',annotations['entities'])\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m example\u001b[39m=\u001b[39mExample\u001b[39m.\u001b[39mfrom_dict(doc,annotations)     \n\u001b[0;32m     16\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     nlp\u001b[39m.\u001b[39mupdate([example],\n\u001b[0;32m     18\u001b[0m     drop\u001b[39m=\u001b[39m\u001b[39m0.1\u001b[39m,\n\u001b[0;32m     19\u001b[0m     sgd\u001b[39m=\u001b[39moptimizer,\n\u001b[0;32m     20\u001b[0m     losses\u001b[39m=\u001b[39mlosses\n\u001b[0;32m     21\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\training\\example.pyx:118\u001b[0m, in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\training\\example.pyx:24\u001b[0m, in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\training\\example.pyx:489\u001b[0m, in \u001b[0;36mspacy.training.example._add_entities_to_doc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python39\\site-packages\\spacy\\training\\iob_utils.py:106\u001b[0m, in \u001b[0;36moffsets_to_biluo_tags\u001b[1;34m(doc, entities, missing)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m token_index \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(start_char, end_char):\n\u001b[0;32m    105\u001b[0m     \u001b[39mif\u001b[39;00m token_index \u001b[39min\u001b[39;00m tokens_in_ents\u001b[39m.\u001b[39mkeys():\n\u001b[1;32m--> 106\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    107\u001b[0m             Errors\u001b[39m.\u001b[39mE103\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    108\u001b[0m                 span1\u001b[39m=\u001b[39m(\n\u001b[0;32m    109\u001b[0m                     tokens_in_ents[token_index][\u001b[39m0\u001b[39m],\n\u001b[0;32m    110\u001b[0m                     tokens_in_ents[token_index][\u001b[39m1\u001b[39m],\n\u001b[0;32m    111\u001b[0m                     tokens_in_ents[token_index][\u001b[39m2\u001b[39m],\n\u001b[0;32m    112\u001b[0m                 ),\n\u001b[0;32m    113\u001b[0m                 span2\u001b[39m=\u001b[39m(start_char, end_char, label),\n\u001b[0;32m    114\u001b[0m             )\n\u001b[0;32m    115\u001b[0m         )\n\u001b[0;32m    116\u001b[0m     tokens_in_ents[token_index] \u001b[39m=\u001b[39m (start_char, end_char, label)\n\u001b[0;32m    117\u001b[0m start_token \u001b[39m=\u001b[39m starts\u001b[39m.\u001b[39mget(start_char)\n",
      "\u001b[1;31mValueError\u001b[0m: [E103] Trying to set conflicting doc.ents: '(0, 15, 'Name')' and '(0, 4, 'Location')'. A token can only be part of one entity, so make sure the entities you're setting don't overlap. To work with overlapping entities, consider using doc.spans instead."
     ]
    }
   ],
   "source": [
    "from spacy.training import Example\n",
    "with nlp.disable_pipes(*other_pipes):\n",
    "    optimizer = nlp.begin_training()\n",
    "    for batch in spacy.util.minibatch(train_data,size=2):\n",
    "        random.shuffle(train_data)\n",
    "        losses={}\n",
    "        index=0\n",
    "        for text, annotations in batch:\n",
    "            doc=nlp.make_doc(text)\n",
    "\n",
    "            # print('BEFORE: ',annotations['entities'])\n",
    "            # annotations['entities']=filter_spans([annotations['entities']])\n",
    "            # print('AFTER: ',annotations['entities'])\n",
    "            \n",
    "            example=Example.from_dict(doc,annotations)     \n",
    "            try:\n",
    "                nlp.update([example],\n",
    "                drop=0.1,\n",
    "                sgd=optimizer,\n",
    "                losses=losses\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "        print(\"LOSSES: \",losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.to_disk('nlp_model')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the saved model into nlp_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp_model=spacy.load('nlp_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Manisha Bharti Software Automation Engineer  Pune, Maharashtra - Email me on Indeed: indeed.com/r/Manisha-Bharti/3573e36088ddc073  • 3.5 years of professional IT experience in Banking and Finance domain and currently working as Software Automation Engineer in Infosys Limited, Pune. • Have experience in accounts and customers domain in banking. • Woking on SOA technology. • Hands on experience of 2+ years in Oracle 11g • 2.9 years of professional experience in Middleware Testing and Functional Testing • 4 months of experience with UiPath. • Experience on GUI and API testing on HP UFT • Working on agile methodology where involved as a senior tester. • Involved in various STLC stages including Test Planning, Test analysis, Test Execution, Defect management and Test reporting. • Possess sound knowledge of SQL, STLC, Testing Procedures, HP ALM, HP UFT, HP SV, SOAP UI, JIRA, JENKINS, CICD, UiPath. • Involved in various client presentation.  Training & Achievement  Title: Infosys E&R Training Description: Has undergone E&R training in Infosys Limited (Mysore) in Microsoft. Net Stream. There I had been explored SQL, RDBMS, OOPS, Mainframes, Software Testing and Software Engineering. Has been trained in Automation Testing Tools used- Eclipse, UFT, RPT, SQL Server Studio Received two times FS INSTA award from Infosys for excellence in work in automation and team support Got Appreciation from Project Manager for root cause analysis of defects Got Client Appreciations for successful execution in releases. ( Almost 240 service operations go live in a year.)  Willing to relocate to: Pune, Maharashtra  WORK EXPERIENCE  NOT WORKING  Software Automation Engineer  Infosys Limited -  August 2014 to July 2017  ->Worked as an software automation tester more than 3 years. ->Working experience in Agile methodology.  https://www.indeed.com/r/Manisha-Bharti/3573e36088ddc073?isid=rex-download&ikw=download-top&co=IN   ->Robotic Process automation certified.(UiPath) ->Involved in CICD implementation in projects. ->Having strong knowledge about HP UFT/QTP,HP ALM/QC,JIRA.JENKINS,SQL.  System Engineer Trainee  Infosys Limited -  February 2014 to July 2014  CORE COMPETENCIES Technology: Service Oriented Architecture (SOA) Languages: • • SQL (Oracle DB) • • VB Scripting • .NET • STLC: • Test Planning • • Requirement Analysis • • Test Scenario • • Test Case Preparation • • Test Case Execution • • Defect Logging • Testing: • Functional Testing • • Middleware Testing • Regression Testing • GUI testing & API testing• Languages VB Scripting, JAVA  Web Technologies ASP.Net, XML, HTML Databases SQL Server 2008/2005, ORACLE 11g  Database Connectivity ODBC Distributed Computing Web Services, API, Windows Services Modelling Tools Microsoft Vision  EDUCATION  B.Tech in CSE  Meghnad saha institute of technology  2013  SKILLS  Uft/qtp,alm/qc,jira,jenkins,automation testing,cicd,service vitualization,uipath    ADDITIONAL INFORMATION  Operating Systems Windows 10 / 8 / 7 / Vista / XP  Domains Banking and Finance  Frameworks Data driven framework, Keyword driven framework  Tools HP-UFT, HP-SV, HP-ALM/QC, SOAP UI, JENKINS, UiPath  Methodologies STLC, Agile and waterfall.  Project Management Tools JIRA  TECHNICAL SKILLS  • 2.5+years of professional experience in SQL. • Has hands on experience on Oracle DB (Oracle 11g) • Has extensive knowledge of Testing Procedures and various phases of Testing Has 2+ years of experience on QC/ALM • Has 2+ years working experience in API & GUI testing using HP UFT. Has 4 months of experience with uiPath • Has 2+ years working experience in SOAP UI. • Has 1.5+ year working experience on service virtualization using HP SV tool. • Has 6 months working experience in JIRA during work under agile methodology. • Has undergone Infosys Training in .Net Testing. • Has knowledge about CICD (Continuous integration and continuous delivery)  PROJECT UNDERTAKEN  DI-Middleware testing (August 14 - July 17) Domain- Accounts and customer. Client- ABN AMRO Bank (Netherland's bank) Project Name- ESB (Enterprise service bus) • Tools- ALM, SQL Developer, HP UFT, HP SV, SOAP UI, JENKINS, JIRA. In this project, we were validating end to end communication of consumer & provider via ESB. What consumer actually sent to the Provider and how provider responds to the consumer. Testing included System Integration Testing, Regression Testing, GUI Testing and Reports.  Responsibilities-  Automation work • Preparing automation scripts using HP UFT tool where focus on Middleware logging as per the ESB behavior. • Integrating all automation scripts with the ALM so that on one click we are able to execute test cases and collecting all test results and logged defects in ALM without any manual efforts. Manual Work -    • Requirement analysis and Test Planning. • Test Scenarios preparation for various functionalities as per the Requirement. • Test Cases Creation and their execution for various functionalities of ESB and different provider services • Prioritization of test cases as per the business requirement • Test Data Preparation as per the requirement using HP ALM. • Defect logging in case of any unusual behavior of the solution. • Preparing Weekly Progress Reports. • Leading the defect call • Virtualizing services using HP SV tool and deploy on central server so that in downtime testing should not be impacted. CICD-JENKINS: • Involve in Continuous integration and continuous deployment strategy, with the help of JENKINS & UFT (automation scri pt integrated with HP ALM) successfully implemented for currently working project.  TDM implantation in CICD pipeline.  Presentation - Direct communication to clients: • Present my team and our work to the client directly. (including CICD and TDM job implementation in the same)  uiPath exposure within same project: • Convert existing/new projects which are using UFT for automation into uiPath based automation. • Do the feasibility analysis for the conversion and come up with a plan to convert maximum artifacts with minimum efforts • Setup basic skeleton for the new project\n",
      "NAME                          -Manisha Bharti\n"
     ]
    }
   ],
   "source": [
    "print(train_data[15][0])\n",
    "doc=nlp_model(train_data[15][0])\n",
    "for ent in doc.ents:\n",
    "    print(f'{ent.label_.upper():{30}}-{ent.text}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit (microsoft store)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "231b793e72510f63263cf75731d759da8776d8c74c3ed2dbefa0f0827e58fa6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
